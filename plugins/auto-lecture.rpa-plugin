{
  "packageInfo": {
    "name": "Auto Lecture",
    "version": "1.0.0",
    "type": "rpa-plugin"
  },
  "files": {
    "plugin.json": "{\n  \"id\": \"plugin-auto-lecture\",\n  \"name\": \"Auto Lecture\",\n  \"description\": \"Auto Lecture\",\n  \"longDescription\": \"Auto Lecture\",\n  \"category\": [\n    \"AI\",\n    \"API Integration\"\n  ],\n  \"version\": \"1.0.0\",\n  \"author\": \"AgentOuro\",\n  \"type\": \"pluginNode\",\n  \"nodeType\": \"pluginNode\",\n  \"color\": \"#6b7280\",\n  \"createdAt\": \"2025-10-29T06:08:39.914Z\",\n  \"updatedAt\": \"2025-11-02T08:01:39.194Z\",\n  \"timeout_mode\": \"long-running\",\n  \"inputFields\": [\n    {\n      \"id\": \"field_1761718057425\",\n      \"type\": \"text\",\n      \"label\": \"Input 1\",\n      \"variableName\": \"topic\",\n      \"required\": true,\n      \"defaultValue\": null\n    },\n    {\n      \"id\": \"field_1761718062593\",\n      \"type\": \"llmConfig\",\n      \"label\": \"Input 2\",\n      \"variableName\": \"LLM\",\n      \"required\": true,\n      \"defaultValue\": null\n    },\n    {\n      \"id\": \"field_1761718069286\",\n      \"type\": \"text\",\n      \"label\": \"Input 3\",\n      \"variableName\": \"slideapi\",\n      \"required\": true,\n      \"defaultValue\": null\n    },\n    {\n      \"id\": \"field_1761718075729\",\n      \"type\": \"text\",\n      \"label\": \"Input 4\",\n      \"variableName\": \"evnapi\",\n      \"required\": true,\n      \"defaultValue\": null\n    },\n    {\n      \"id\": \"field_1761718081587\",\n      \"type\": \"text\",\n      \"label\": \"Input 5\",\n      \"variableName\": \"numslide\",\n      \"required\": false,\n      \"defaultValue\": null\n    }\n  ],\n  \"outputFields\": [\n    {\n      \"id\": \"output_1761718089026\",\n      \"type\": \"file\",\n      \"label\": \"Output 1\",\n      \"variableName\": \"result\"\n    }\n  ],\n  \"globalVariableAccess\": []\n}",
    "index.py": "def run(inputs, ctx):\n    \"\"\"\n    AI Lecture Generator - Creates presentation slides with audio narration and video\n\n    This version uses system tools installed via custom Dockerfile instead of ctx.execute_command()\n\n    Required inputs:\n    - topic: The lecture topic (string)\n    - LLM: LLM config {baseUrl, apiKey, model}\n    - slideapi: SlideSpeak API key (string)\n    - evnapi: ElevenLabs API key (string)\n    - numslide: Number of slides (int, default: 8)\n    \"\"\"\n\n    import asyncio\n    import base64\n    import datetime\n    import io\n    import json\n    import os\n    import random\n    import re\n    import shutil\n    import subprocess\n    import tempfile\n    import time\n    import zipfile\n    from pathlib import Path\n\n    # ========================================================================\n    # DATA MODELS\n    # ========================================================================\n\n    class SlideContent:\n        def __init__(self, title, layout, item_amount, content, speaker_notes, images=None):\n            self.title = title\n            self.layout = layout\n            self.item_amount = item_amount\n            self.content = content\n            self.speaker_notes = speaker_notes\n            self.images = images or []\n\n    class CharacterAlignment:\n        def __init__(self, characters, character_start_times_seconds, character_end_times_seconds):\n            self.characters = characters\n            self.character_start_times_seconds = character_start_times_seconds\n            self.character_end_times_seconds = character_end_times_seconds\n\n    class TTSWithTimestamps:\n        def __init__(self, audio_base64, alignment, **kwargs):\n            self.audio_base64 = audio_base64\n            self.alignment = CharacterAlignment(**alignment)\n\n    class LectureConfig:\n        def __init__(self, **kwargs):\n            self.llm_base_url = kwargs.get('llm_base_url')\n            self.llm_api_key = kwargs.get('llm_api_key')\n            self.llm_model = kwargs.get('llm_model')\n            self.slidespeak_api_key = kwargs.get('slidespeak_api_key')\n            self.elevenlabs_api_key = kwargs.get('elevenlabs_api_key')\n            self.elevenlabs_voice_id = kwargs.get('elevenlabs_voice_id', '21m00Tcm4TlvDq8ikWAM')\n            self.num_slides = kwargs.get('num_slides', 8)\n            self.tone = kwargs.get('tone', 'educational')\n            self.audience = kwargs.get('audience', 'students')\n            self.generate_video = kwargs.get('generate_video', True)\n            self.include_subtitles = kwargs.get('include_subtitles', True)\n            self.video_resolution = kwargs.get('video_resolution', '1920x1080')\n\n    class GenerationResult:\n        def __init__(self):\n            self.slides = []\n            self.pptx_url = \"\"\n            self.pptx_content = b\"\"\n            self.audio_files = {}\n            self.audio_alignments = {}\n            self.video_path = None\n            self.subtitle_content = None\n            self.task_id = \"\"\n\n    # ========================================================================\n    # LLM CONTENT GENERATION\n    # ========================================================================\n\n    async def generate_slides_with_llm(topic, config, ctx):\n        ctx.log(\"Generating slide content with LLM...\")\n        ctx.heartbeat()\n\n        system_prompt = \"\"\"You are an expert educational content creator. Generate a structured presentation outline with speaker notes.\n\nFor each slide, provide:\n1. title: Clear, concise slide title (max 60 characters)\n2. layout: Choose from [items, timeline, comparison, big-number, thanks]\n3. item_amount: Number of items (must match layout constraints)\n4. content: Structured content for the slide\n5. speaker_notes: Natural speech script for narration (60-90 seconds when spoken)\n\nCRITICAL: Return ONLY a valid JSON array. No markdown code blocks.\"\"\"\n\n        user_prompt = f\"\"\"Create a {config.num_slides}-slide presentation about: {topic}\n\nTarget audience: {config.audience}\nTone: {config.tone}\n\nReturn as JSON array:\n[\n  {{\n    \"title\": \"Introduction to Topic\",\n    \"layout\": \"items\",\n    \"item_amount\": 3,\n    \"content\": \"Point 1\\\\nPoint 2\\\\nPoint 3\",\n    \"speaker_notes\": \"Welcome everyone...\"\n  }}\n]\"\"\"\n\n        request_data = {\n            \"url\": config.llm_base_url,\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Authorization\": f\"Bearer {config.llm_api_key}\",\n                \"Content-Type\": \"application/json\"\n            },\n            \"body\": json.dumps({\n                \"model\": config.llm_model,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                \"temperature\": 0.7\n            })\n        }\n\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(None, ctx.http_fetch, request_data)\n        result = response.get(\"data\")\n\n        if not result:\n            raise Exception(\"LLM API request failed\")\n\n        content = result['choices'][0]['message']['content'].strip()\n\n        # Clean markdown blocks\n        if content.startswith('```json'):\n            content = content[7:]\n        if content.startswith('```'):\n            content = content[3:]\n        if content.endswith('```'):\n            content = content[:-3]\n        content = content.strip()\n\n        try:\n            slides_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', content, re.DOTALL)\n            if json_match:\n                json_str = json_match.group(0)\n                json_str = re.sub(r',(\\s*[}\\]])', r'\\1', json_str)\n                slides_data = json.loads(json_str)\n            else:\n                raise Exception(\"No valid JSON array found in LLM response\")\n\n        slides = [\n            SlideContent(\n                title=s['title'],\n                layout=s['layout'],\n                item_amount=s['item_amount'],\n                content=s['content'],\n                speaker_notes=s['speaker_notes'],\n                images=s.get('images')\n            )\n            for s in slides_data\n        ]\n\n        ctx.log(f\"Generated {len(slides)} slides\")\n        return slides\n\n    # ========================================================================\n    # SLIDESPEAK API\n    # ========================================================================\n\n    async def get_slidespeak_templates(api_key, ctx):\n        try:\n            request_data = {\n                \"url\": \"https://api.slidespeak.co/api/v1/presentation/templates\",\n                \"method\": \"GET\",\n                \"headers\": {\n                    \"Content-Type\": \"application/json\",\n                    \"X-API-Key\": api_key\n                }\n            }\n\n            loop = asyncio.get_event_loop()\n            response = await loop.run_in_executor(None, ctx.http_fetch, request_data)\n            result = response.get(\"data\")\n\n            if not result:\n                return [\"default\"]\n\n            template_names = [t[\"name\"] for t in result]\n            ctx.log(f\"Available templates: {template_names}\")\n            return template_names\n        except Exception:\n            return [\"default\"]\n\n    async def create_presentation_with_slidespeak(slides, config, ctx):\n        ctx.log(f\"Creating presentation with {len(slides)} slides...\")\n\n        available_templates = await get_slidespeak_templates(config.slidespeak_api_key, ctx)\n        selected_template = random.choice(available_templates)\n        ctx.log(f\"Selected template: {selected_template}\")\n\n        slides_data = [\n            {\n                \"title\": slide.title,\n                \"layout\": slide.layout,\n                \"item_amount\": slide.item_amount,\n                \"content\": slide.content,\n                **({\"images\": slide.images} if slide.images else {})\n            }\n            for slide in slides\n        ]\n\n        payload = {\n            \"template\": selected_template,\n            \"language\": \"ORIGINAL\",\n            \"fetch_images\": True,\n            \"verbosity\": \"standard\",\n            \"include_cover\": True,\n            \"include_table_of_contents\": False,\n            \"add_speaker_notes\": True,\n            \"slides\": slides_data\n        }\n\n        request_data = {\n            \"url\": \"https://api.slidespeak.co/api/v1/presentation/generate/slide-by-slide\",\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Content-Type\": \"application/json\",\n                \"X-API-Key\": config.slidespeak_api_key\n            },\n            \"body\": json.dumps(payload)\n        }\n\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(None, ctx.http_fetch, request_data)\n        result = response.get(\"data\")\n\n        if not result:\n            raise Exception(\"SlideSpeak API request failed\")\n\n        task_id = result['task_id']\n        ctx.log(f\"SlideSpeak task: {task_id}\")\n\n        # Poll for completion\n        max_attempts = 60\n        attempt = 0\n\n        while attempt < max_attempts:\n            await asyncio.sleep(5)\n            attempt += 1\n            ctx.heartbeat()\n\n            status_request = {\n                \"url\": f\"https://api.slidespeak.co/api/v1/task_status/{task_id}\",\n                \"method\": \"GET\",\n                \"headers\": {\"X-API-Key\": config.slidespeak_api_key}\n            }\n\n            status_response = await loop.run_in_executor(None, ctx.http_fetch, status_request)\n            status_data = status_response.get(\"data\")\n\n            if not status_data:\n                continue\n\n            task_status = status_data.get('task_status')\n\n            if task_status == 'SUCCESS':\n                pptx_url = status_data['task_result']['url']\n                ctx.log(f\"Presentation ready: {pptx_url}\")\n                return task_id, pptx_url\n            elif task_status == 'FAILURE':\n                error_msg = status_data.get('task_error', 'Unknown error')\n                raise Exception(f\"SlideSpeak generation failed: {error_msg}\")\n\n        raise Exception(\"SlideSpeak generation timed out\")\n\n    def download_presentation_sync(pptx_url, ctx):\n        ctx.log(f\"Downloading presentation...\")\n        ctx.heartbeat()\n\n        request_data = {\n            \"url\": pptx_url,\n            \"method\": \"GET\",\n            \"headers\": {},\n            \"expect\": \"binary\"\n        }\n\n        response = ctx.http_fetch(request_data)\n\n        if response.get(\"status\") != 200:\n            raise Exception(f\"HTTP {response.get('status')}\")\n\n        artifact_id = response.get(\"artifact\") or response.get(\"data\")\n\n        if isinstance(artifact_id, str):\n            content = ctx.read_bytes(artifact_id)\n        elif isinstance(artifact_id, bytes):\n            content = artifact_id\n        else:\n            raise Exception(f\"Unexpected artifact type: {type(artifact_id)}\")\n\n        ctx.log(f\"Downloaded presentation ({len(content)} bytes)\")\n        return content\n\n    # ========================================================================\n    # TTS AUDIO GENERATION\n    # ========================================================================\n\n    async def generate_audio_with_elevenlabs(text, config, ctx):\n        url = f\"https://api.elevenlabs.io/v1/text-to-speech/{config.elevenlabs_voice_id}/with-timestamps?output_format=mp3_44100_128\"\n\n        request_data = {\n            \"url\": url,\n            \"method\": \"POST\",\n            \"headers\": {\n                \"xi-api-key\": config.elevenlabs_api_key,\n                \"Content-Type\": \"application/json\"\n            },\n            \"json\": {\n                \"text\": text,\n                \"model_id\": \"eleven_monolingual_v1\",\n                \"voice_settings\": {\n                    \"stability\": 0.5,\n                    \"similarity_boost\": 0.75\n                }\n            },\n            \"expect\": \"json\",\n            \"timeoutMs\": 30000\n        }\n\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(None, ctx.http_fetch, request_data)\n\n        if response.get(\"status\") != 200:\n            error_msg = response.get(\"data\", \"Unknown error\")\n            raise Exception(f\"ElevenLabs API error {response.get('status')}: {error_msg}\")\n\n        data = response.get(\"data\")\n        if not data:\n            raise Exception(\"Empty response from ElevenLabs\")\n\n        tts_response = TTSWithTimestamps(**data)\n        audio_bytes = base64.b64decode(tts_response.audio_base64)\n\n        if not audio_bytes or len(audio_bytes) < 100:\n            raise Exception(f\"Invalid audio data (size: {len(audio_bytes)} bytes)\")\n\n        return audio_bytes, tts_response.alignment\n\n    async def generate_all_audio(slides, config, ctx):\n        ctx.log(f\"Generating audio for {len(slides)} slides...\")\n\n        audio_files = {}\n        alignments = {}\n\n        for idx, slide in enumerate(slides):\n            ctx.log(f\"Slide {idx + 1}/{len(slides)}: {slide.title}\")\n            ctx.heartbeat()\n\n            try:\n                audio_bytes, alignment = await generate_audio_with_elevenlabs(\n                    slide.speaker_notes,\n                    config,\n                    ctx\n                )\n                audio_files[idx] = audio_bytes\n                alignments[idx] = alignment\n                ctx.log(f\"✓ Slide {idx + 1} complete ({len(audio_bytes)} bytes)\")\n            except Exception as e:\n                ctx.log(f\"✗ Slide {idx + 1} failed: {str(e)}\")\n\n        successful = len([a for a in audio_files.values() if a])\n        ctx.log(f\"Audio generation: {successful}/{len(slides)} successful\")\n\n        if successful == 0:\n            raise Exception(\"Failed to generate any audio files\")\n\n        return audio_files, alignments\n\n    # ========================================================================\n    # VIDEO ASSEMBLY (using direct subprocess calls)\n    # ========================================================================\n\n    def convert_pptx_to_images(pptx_content, output_dir, ctx):\n        ctx.log(\"Converting PPTX to images...\")\n        ctx.heartbeat()\n\n        pptx_path = output_dir / \"presentation.pptx\"\n        with open(pptx_path, 'wb') as f:\n            f.write(pptx_content)\n\n        # Convert PPTX to PDF using LibreOffice\n        result = subprocess.run(\n            ['soffice', '--headless', '--convert-to', 'pdf', '--outdir', str(output_dir), str(pptx_path)],\n            capture_output=True,\n            timeout=120\n        )\n\n        if result.returncode != 0:\n            raise Exception(f\"PPTX to PDF conversion failed: {result.stderr.decode()}\")\n\n        pdf_path = output_dir / \"presentation.pdf\"\n        if not pdf_path.exists():\n            raise Exception(\"PDF not generated\")\n\n        ctx.heartbeat()\n\n        # Convert PDF to PNG images using pdftoppm\n        result = subprocess.run(\n            ['pdftoppm', '-png', '-r', '150', str(pdf_path), str(output_dir / 'slide')],\n            capture_output=True,\n            timeout=120\n        )\n\n        if result.returncode != 0:\n            raise Exception(f\"PDF to PNG conversion failed: {result.stderr.decode()}\")\n\n        image_files = sorted(output_dir.glob(\"slide-*.png\"))\n\n        if not image_files:\n            raise Exception(\"No PNG files generated\")\n\n        ctx.log(f\"Converted to {len(image_files)} images\")\n        return image_files\n\n    def get_audio_duration(audio_bytes, temp_dir):\n        try:\n            audio_path = Path(temp_dir) / f\"temp_audio_{int(time.time())}.mp3\"\n            with open(audio_path, 'wb') as f:\n                f.write(audio_bytes)\n\n            result = subprocess.run(\n                ['ffprobe', '-i', str(audio_path), '-show_entries', 'format=duration', '-v', 'quiet', '-of', 'csv=p=0'],\n                capture_output=True,\n                timeout=10\n            )\n\n            if result.returncode == 0 and result.stdout.strip():\n                duration = float(result.stdout.strip())\n                os.unlink(audio_path)\n                return duration\n            else:\n                if audio_path.exists():\n                    os.unlink(audio_path)\n                return 8.0\n        except Exception:\n            return 8.0\n\n    def generate_subtitles(slides, audio_files, alignments, ctx):\n        ctx.log(\"Generating subtitles...\")\n\n        srt_content = []\n        subtitle_index = 1\n        current_time_offset = 0.0\n\n        MAX_CHARS_PER_LINE = 42\n        MAX_DURATION = 6.0\n\n        for idx, slide in enumerate(slides):\n            if idx not in audio_files or not audio_files[idx] or idx not in alignments:\n                continue\n\n            alignment = alignments[idx]\n            words_with_timing = extract_words_with_timing(slide.speaker_notes, alignment)\n            chunks = group_words_into_chunks(words_with_timing, MAX_CHARS_PER_LINE, MAX_DURATION)\n\n            for chunk in chunks:\n                if not chunk:\n                    continue\n\n                start_time = chunk[0][1] + current_time_offset\n                end_time = chunk[-1][2] + current_time_offset\n                chunk_text = ' '.join(word[0] for word in chunk)\n\n                srt_content.append(f\"{subtitle_index}\")\n                srt_content.append(f\"{format_srt_time(start_time)} --> {format_srt_time(end_time)}\")\n                srt_content.append(chunk_text)\n                srt_content.append(\"\")\n\n                subtitle_index += 1\n\n            if words_with_timing:\n                current_time_offset += words_with_timing[-1][2]\n\n        return \"\\n\".join(srt_content)\n\n    def extract_words_with_timing(text, alignment):\n        words_with_timing = []\n        current_word = []\n        word_start_time = None\n        word_end_time = None\n\n        for char, start, end in zip(\n            alignment.characters,\n            alignment.character_start_times_seconds,\n            alignment.character_end_times_seconds\n        ):\n            if char.strip() and not re.match(r'[.,!?;:\\-—()\\[\\]{}\"\\']', char):\n                if word_start_time is None:\n                    word_start_time = start\n                current_word.append(char)\n                word_end_time = end\n            else:\n                if current_word:\n                    word_text = ''.join(current_word)\n                    words_with_timing.append((word_text, word_start_time, word_end_time))\n                    current_word = []\n                    word_start_time = None\n                    word_end_time = None\n\n        if current_word:\n            word_text = ''.join(current_word)\n            words_with_timing.append((word_text, word_start_time, word_end_time))\n\n        return words_with_timing\n\n    def group_words_into_chunks(words, max_chars, max_duration):\n        chunks = []\n        current_chunk = []\n        current_text_length = 0\n        chunk_start_time = None\n\n        for word, start, end in words:\n            word_length = len(word)\n\n            if chunk_start_time is None:\n                chunk_start_time = start\n\n            chunk_duration = end - chunk_start_time\n            new_length = current_text_length + word_length + (1 if current_chunk else 0)\n\n            if current_chunk and (new_length > max_chars or chunk_duration > max_duration):\n                chunks.append(current_chunk)\n                current_chunk = []\n                current_text_length = 0\n                chunk_start_time = start\n\n            current_chunk.append((word, start, end))\n            current_text_length += word_length + (1 if len(current_chunk) > 1 else 0)\n\n        if current_chunk:\n            chunks.append(current_chunk)\n\n        return chunks\n\n    def format_srt_time(seconds):\n        hours = int(seconds // 3600)\n        minutes = int((seconds % 3600) // 60)\n        secs = int(seconds % 60)\n        millis = int((seconds % 1) * 1000)\n        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n\n    def assemble_video_with_ffmpeg(images, audio_files, slides, alignments, output_path, include_subtitles, resolution, ctx):\n        ctx.log(\"Assembling video with FFmpeg...\")\n\n        if not audio_files:\n            raise Exception(\"Cannot create video without audio files\")\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Save audio files\n            audio_paths = []\n            audio_durations = []\n\n            for idx in sorted(audio_files.keys()):\n                if not audio_files[idx]:\n                    continue\n\n                audio_path = temp_path / f\"audio_{idx:03d}.mp3\"\n                with open(audio_path, 'wb') as f:\n                    f.write(audio_files[idx])\n\n                audio_paths.append(audio_path)\n                audio_durations.append(get_audio_duration(audio_files[idx], str(temp_path)))\n\n            # Generate subtitles\n            subtitle_path = None\n            srt_content = None\n            if include_subtitles and alignments:\n                srt_content = generate_subtitles(slides, audio_files, alignments, ctx)\n                subtitle_path = temp_path / \"subtitles.srt\"\n                with open(subtitle_path, 'w', encoding='utf-8') as f:\n                    f.write(srt_content)\n\n            if not audio_paths:\n                raise Exception(\"No valid audio files\")\n\n            # Merge audio\n            merged_audio_path = temp_path / \"merged_audio.mp3\"\n\n            if len(audio_paths) == 1:\n                shutil.copy(audio_paths[0], merged_audio_path)\n            else:\n                audio_concat_path = temp_path / \"audio_concat.txt\"\n                concat_lines = [f\"file '{str(ap.absolute())}'\" for ap in audio_paths]\n\n                with open(audio_concat_path, 'w', encoding='utf-8') as f:\n                    f.write('\\n'.join(concat_lines) + '\\n')\n\n                ctx.heartbeat()\n\n                result = subprocess.run(\n                    ['ffmpeg', '-f', 'concat', '-safe', '0', '-i', str(audio_concat_path),\n                     '-c:a', 'libmp3lame', '-b:a', '192k', '-ar', '44100', str(merged_audio_path), '-y'],\n                    capture_output=True,\n                    timeout=300\n                )\n\n                if result.returncode != 0:\n                    raise Exception(f\"Audio merge failed: {result.stderr.decode()}\")\n\n            # Parse resolution\n            width, height = resolution.split('x')\n\n            # OPTIMIZED APPROACH: Generate per-slide videos, then concatenate\n            # This uses much less memory than loading all slides at once\n            ctx.log(f\"Generating {len(audio_durations)} slide videos...\")\n            ctx.heartbeat()\n\n            segment_paths = []\n            for idx, (img, audio_path, duration) in enumerate(zip(\n                images[:len(audio_durations)],\n                audio_paths,\n                audio_durations\n            )):\n                segment_path = temp_path / f\"segment_{idx:03d}.mp4\"\n\n                # Generate single slide video with audio\n                segment_args = [\n                    'ffmpeg',\n                    '-loop', '1',\n                    '-i', str(img),\n                    '-i', str(audio_path),\n                    '-vf', f\"scale={width}:{height}:force_original_aspect_ratio=decrease,\"\n                           f\"pad={width}:{height}:(ow-iw)/2:(oh-ih)/2,setsar=1,fps=30\",\n                    '-c:v', 'libx264',\n                    '-preset', 'medium',\n                    '-crf', '23',\n                    '-c:a', 'aac',\n                    '-b:a', '192k',\n                    '-t', str(duration),\n                    '-pix_fmt', 'yuv420p',  # Ensure consistent pixel format\n                    str(segment_path),\n                    '-y'\n                ]\n\n                result = subprocess.run(segment_args, capture_output=True, timeout=120)\n\n                if result.returncode != 0:\n                    stderr_lines = result.stderr.decode().strip().split('\\n')\n                    error_msg = '\\n'.join(stderr_lines[-30:])\n                    raise Exception(f\"Segment {idx} creation failed: {error_msg}\")\n\n                segment_paths.append(segment_path)\n                ctx.log(f\"Generated segment {idx+1}/{len(audio_durations)}\")\n                ctx.heartbeat()\n\n            # Concatenate all segments\n            ctx.log(\"Concatenating segments...\")\n            ctx.heartbeat()\n\n            concat_list_path = temp_path / \"concat_list.txt\"\n            with open(concat_list_path, 'w', encoding='utf-8') as f:\n                for seg in segment_paths:\n                    f.write(f\"file '{seg.absolute()}'\\n\")\n\n            # Concatenate without re-encoding (fast and lossless)\n            concat_args = [\n                'ffmpeg',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', str(concat_list_path),\n                '-c', 'copy',  # Copy streams without re-encoding\n            ]\n\n            # Add subtitles if needed (requires re-encoding)\n            if include_subtitles and subtitle_path:\n                # Must re-encode to add subtitles\n                concat_args = [\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_list_path),\n                    '-i', str(subtitle_path),\n                    '-c:v', 'copy',  # Copy video\n                    '-c:a', 'copy',  # Copy audio\n                    '-c:s', 'mov_text',  # Encode subtitles\n                    '-metadata:s:s:0', 'language=eng',\n                ]\n\n            concat_args.extend([\n                '-movflags', '+faststart',\n                str(output_path),\n                '-y'\n            ])\n\n            result = subprocess.run(concat_args, capture_output=True, timeout=300)\n\n            ctx.log(f\"FFmpeg return code: {result.returncode}\")\n\n            # Check if output file was created\n            if output_path.exists():\n                file_size = output_path.stat().st_size\n                ctx.log(f\"Output file created: {file_size} bytes\")\n            else:\n                ctx.log(\"WARNING: Output file not created!\")\n\n            if result.returncode != 0:\n                # Get last 50 lines of stderr (actual errors are usually at the end)\n                stderr_lines = result.stderr.decode().strip().split('\\n')\n                error_msg = '\\n'.join(stderr_lines[-50:])\n                raise Exception(f\"Video assembly failed (code {result.returncode}):\\n{error_msg}\")\n\n            ctx.log(f\"Video created successfully: {output_path}\")\n            return output_path, srt_content\n\n    # ========================================================================\n    # OUTPUT PACKAGING\n    # ========================================================================\n\n    def create_output_zip(pptx_content, audio_files, slides, video_path, subtitle_content, ctx):\n        ctx.log(\"Creating output ZIP...\")\n        zip_buffer = io.BytesIO()\n\n        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:\n            zf.writestr(\"presentation.pptx\", pptx_content)\n\n            for idx, audio_bytes in audio_files.items():\n                if audio_bytes:\n                    filename = f\"audio/slide_{idx+1:03d}.mp3\"\n                    zf.writestr(filename, audio_bytes)\n\n            if video_path and video_path.exists():\n                with open(video_path, 'rb') as vf:\n                    zf.writestr(\"lecture_video.mp4\", vf.read())\n\n            ctx.heartbeat()\n\n            if subtitle_content:\n                zf.writestr(\"subtitles.srt\", subtitle_content)\n\n            manifest = {\n                \"generated_at\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"total_slides\": len(slides),\n                \"audio_files\": len([a for a in audio_files.values() if a]),\n                \"has_video\": video_path is not None and video_path.exists(),\n                \"has_subtitles\": subtitle_content is not None,\n                \"slides\": [\n                    {\n                        \"index\": idx + 1,\n                        \"title\": slide.title,\n                        \"layout\": slide.layout,\n                        \"has_audio\": idx in audio_files and bool(audio_files[idx])\n                    }\n                    for idx, slide in enumerate(slides)\n                ]\n            }\n            zf.writestr(\"manifest.json\", json.dumps(manifest, indent=2))\n\n        return zip_buffer.getvalue()\n\n    # ========================================================================\n    # MAIN ORCHESTRATION\n    # ========================================================================\n\n    async def generate_lecture(topic, config, ctx):\n        result = GenerationResult()\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"STEP 1: Generating slide content with LLM\")\n        ctx.log(\"=\" * 70)\n        result.slides = await generate_slides_with_llm(topic, config, ctx)\n        ctx.heartbeat()\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"STEP 2: Creating presentation with SlideSpeak\")\n        ctx.log(\"=\" * 70)\n        result.task_id, result.pptx_url = await create_presentation_with_slidespeak(\n            result.slides, config, ctx\n        )\n        ctx.heartbeat()\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"STEP 3: Downloading presentation\")\n        ctx.log(\"=\" * 70)\n        loop = asyncio.get_event_loop()\n        result.pptx_content = await loop.run_in_executor(\n            None, download_presentation_sync, result.pptx_url, ctx\n        )\n        ctx.heartbeat()\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"STEP 4: Generating audio narration\")\n        ctx.log(\"=\" * 70)\n        result.audio_files, result.audio_alignments = await generate_all_audio(\n            result.slides, config, ctx\n        )\n        ctx.heartbeat()\n\n        if config.generate_video and result.audio_files:\n            ctx.log(\"=\" * 70)\n            ctx.log(\"STEP 5: Assembling video\")\n            ctx.log(\"=\" * 70)\n\n            with tempfile.TemporaryDirectory() as temp_dir:\n                temp_path = Path(temp_dir)\n\n                images = await loop.run_in_executor(\n                    None, convert_pptx_to_images, result.pptx_content, temp_path, ctx\n                )\n                ctx.heartbeat()\n\n                video_output = Path(temp_dir) / \"lecture_video.mp4\"\n\n                result.video_path, result.subtitle_content = await loop.run_in_executor(\n                    None,\n                    assemble_video_with_ffmpeg,\n                    images,\n                    result.audio_files,\n                    result.slides,\n                    result.audio_alignments,\n                    video_output,\n                    config.include_subtitles,\n                    config.video_resolution,\n                    ctx\n                )\n\n                if result.video_path and result.video_path.exists():\n                    persistent_video = Path(tempfile.gettempdir()) / f\"lecture_video_{int(time.time())}.mp4\"\n                    shutil.copy(result.video_path, persistent_video)\n                    result.video_path = persistent_video\n\n                ctx.heartbeat()\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"LECTURE GENERATION COMPLETE\")\n        ctx.log(\"=\" * 70)\n\n        return result\n\n    # ========================================================================\n    # PLUGIN EXECUTION\n    # ========================================================================\n\n    try:\n        topic = inputs.get(\"topic\")\n\n        llm_cfg = inputs.get(\"LLM\", {})\n        llm_base_url = llm_cfg.get(\"baseUrl\", \"https://openrouter.ai/api/v1\")\n        if not llm_base_url.endswith(\"/chat/completions\"):\n            llm_base_url = llm_base_url.rstrip(\"/\") + \"/chat/completions\"\n\n        llm_api_key = llm_cfg.get(\"apiKey\")\n        llm_model = llm_cfg.get(\"model\", \"openai/gpt-4o-mini\")\n\n        slidespeak_api_key = inputs.get(\"slideapi\")\n        elevenlabs_api_key = inputs.get(\"evnapi\")\n        num_slides = inputs.get(\"numslide\", 8)\n\n        if not topic:\n            raise Exception(\"Missing required input: topic\")\n        if not llm_api_key:\n            raise Exception(\"Missing required input: LLM.apiKey\")\n        if not slidespeak_api_key:\n            raise Exception(\"Missing required input: slideapi\")\n        if not elevenlabs_api_key:\n            raise Exception(\"Missing required input: evnapi\")\n\n        ctx.log(\"=\" * 70)\n        ctx.log(\"AI LECTURE GENERATOR\")\n        ctx.log(\"=\" * 70)\n        ctx.log(f\"Topic: {topic[:100]}...\")\n        ctx.log(f\"Slides: {num_slides}\")\n\n        config = LectureConfig(\n            llm_base_url=llm_base_url,\n            llm_api_key=llm_api_key,\n            llm_model=llm_model,\n            slidespeak_api_key=slidespeak_api_key,\n            elevenlabs_api_key=elevenlabs_api_key,\n            num_slides=num_slides,\n            generate_video=True,\n            include_subtitles=True,\n            video_resolution=\"1920x1080\"\n        )\n\n        # Use system-level safe_async to handle event loop\n        result = safe_async(generate_lecture(topic, config, ctx))\n\n        ctx.log(\"Packaging output...\")\n        ctx.heartbeat()\n        zip_content = create_output_zip(\n            result.pptx_content,\n            result.audio_files,\n            result.slides,\n            result.video_path,\n            result.subtitle_content,\n            ctx\n        )\n\n        artifact_handle = ctx.save_bytes(data=zip_content, suffix=\".zip\")\n        artifact_bytes = ctx.read_bytes(artifact_handle)\n\n        ctx.log(f\"Output ZIP created ({len(artifact_bytes)} bytes)\")\n        ctx.log(\"Lecture generation completed!\")\n\n        return {\n            \"name\": \"lecture_output.zip\",\n            \"content\": base64.b64encode(artifact_bytes).decode('utf-8'),\n            \"extension\": \".zip\",\n            \"type\": \"application/zip\",\n            \"size\": len(artifact_bytes)\n        }\n\n    except Exception as e:\n        ctx.log(f\"ERROR: {str(e)}\")\n        raise"
  }
}